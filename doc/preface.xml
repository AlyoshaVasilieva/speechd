<preface>

  <title>Preface</title>

  <para>
    Speech Daemon project comes to provide a device independent layer for
    speech synthesis. It should provide a simple interface for client
    applications (applications, that want to speak) as well as for device
    driver modules (for particular speech synthesis).
  </para>

  <para>
   High quality speech synthesis has been available for
   a long time and now it's usable even by ordinary users on their
   home PC's. It comes sometimes as a necesity, sometimes
   as a good feature for programs to provide speech output.
   There is a wide field of possible uses from educational
   software, through specialized systems (hospitals, laboratories)
   and friendlier user interfaces to software for our visually impaired
   fellows. This is exactly where Speech Deamon came from.
  </para>
  <para>
   Ordinary users don't meet speech synthesis too often
   and I would say that it's power is often underpreciated.
   But for blind people it's one of the few ways how to communicate
   with computer. They are living with speech synthesis each day
   so it should be no surprise that it were they who realised
   for the first time that the current situation with speech synthesis
   on the GNU/Linux system is a great chaos.
  </para>
  <para>
   There are different speech synthetisers with different capabilities.
   Some of them are hardware, some of them are software. Some of them
   are Free Software and are available on the internet. However, none
   of them is preinstalled in one of the widely used GNU/Linux
   distributions. Programmers have really hard times when they want
   to make their program speak because they need to find some suitable
   synthetiser (long hours of experiments and so on...) and then
   make it work with their program. They often need to write output
   device drivers for these programs or hardware devices and are
   doing it again and again. You can imagine it all fails when an
   innocent user executes two programs with speech output at 
   once -- if they even start both (what I doubt), they will be
   shouting one over the other. This makes it very hard for programmers
   to insert speech support to their programs (for blind users or
   simple for making better user interface) and it's one of the reasons
   we still don't fully exploit what speech synthesis technology offers.
  </para>
  <para>
   In an ideal world, programmers could use similar commands for
   speech synthesis as they do for normal text` output (printf, 
   puts, ...). In an ideal world, there would be some speech_printf()
   that would take care of saying your message in the right time without
   interumping others, without you beeing obligated to take care of
   how exactly the communication with speech synthetiser is implemented
   and without you having to worry about what synthetiser to use and if
   it's available. In an ideal world, there would be some speech synthetiser
   in each GNU/Linux distribution and some speech deamon taking care of all
   applications that want to speak, allowing user to configure speech
   parameters and providing simple interface (as speech_printf())
   through some shared library for programmers. It will be a long
   way until we get archieve this state of things, but with Speech Deamon,
   we are taking the first steps...
  </para>
  
</preface>
