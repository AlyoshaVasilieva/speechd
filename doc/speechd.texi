\input texinfo   @c -*-texinfo-*-
@c %**start of header
@setfilename speechd.info
@settitle Speech Deamon -- Easy acces to speech synthesis
@finalout
@setchapternewpage odd
@c %**end of header

@syncodeindex pg cp
@syncodeindex fn cp
@syncodeindex vr cp

@include version.texi
     
@copying
This manual is for Speech Daemon, version @value{VERSION}.

Copyright @copyright{} 2001, 2002, 2003 ???

@quotation
Permission is granted to ...
@end quotation
@end copying

@dircategory Sound
@dircategory Development

@direntry
* Speech Deamon: (speechd).       Speech Deamon.
@end direntry


@titlepage
@title Speech Deamon -- Easy acces to speech synthesis
@subtitle Mastering the Babylon of TTS'
@subtitle Edition @value{EDITION}, for Speech Deamon @value{VERSION}
@subtitle @value{UPDATED}
@author by Tomas Cerha <@email{cerha@@brailcom.cz}>, Hynek Hanke <@email{hanke@@volny.cz}>
@author and Czech Free Software organization <@email{info@@freesoft.cz}>

@page
@vskip 0pt plus 1filll

@insertcopying

@end titlepage

@c So the toc is printed in the right place.
@contents

@ifnottex
@node Top, Instructions, (dir), (dir)

This file documents the @code{speechd} client/server application 
that attempts to provide a common interface to different synthesizers.

@insertcopying
@end ifnottex

@menu
* Instructions::             How to read this manual.
* Introduction::             What is Speech Deamon.
* Invoking::                 How to start Speech Deamon.
* Internal structure::       How does Speech Deamon work.
* Public API::               How to use Speech Deamon in your programs.
* SSIP::                     Speech Synthesis Independent Protocol.
* Priorities::               Description, guidelines how to use them.
* Multiple output modules::  Using different synthesizers.
* Messsage history::         Handling history of arrived messages.
* Speech parameters::        Settings that affect the way Speech Deamon speaks.
* Configuration::            How to configure Speech Deamon.

* Copying::                  GNU General Public License.

* Concept index::            Index of concepts.
@end menu

@node Instructions, Introduction, Top, Top
@chapter How to read this manual
@cindex How to read

-> there should be a simple map of the manual and where to search for different concepts

@node Introduction, Invoking, Instructions, Top
@chapter Introduction

@menu
* Why and how::                 Why Speech Deamon?  Philosophy, motivation...
* Current state::               Speaking software today.
* Basic design::                How does it work?
* User::                        Speech Deamon from user's point of view.
* Programmer::                  Speech Deam from programmer's poing of view.
@end menu

@node Why and how, Current state, Introduction, Introduction
@section Why and how
@cindex Basic ideas, Motivation
@cindex Philosophy

Speech Daemon project comes to provide a device independent layer for
speech synthesis. It should provide a simple interface for client
applications (applications, that want to speak) as well as for device
driver modules (for particular speech synthesis).

High quality speech synthesis has been available for a long time and now
it's usable even by ordinary users on their home PC's. It comes sometimes
as a necesity, sometimes as a good feature for programs to provide speech
output.  There is a wide field of possible uses from educational software,
through specialized systems (hospitals, laboratories, telephony servers).
For visually impaired users it is one of the two essential ways of getting
the output from computer (the second one is Braille display). That's also
where Speech Deamon comes from.

There are different speech synthesisers with different capabilities.  Some
of them are hardware, some of them are software.  Some of them are Free
Software and are are available on the Internet.  However, none of them is
preinstalled in one of the widely used GNU/Linux distributions.
Programmers have really hard times when they want to make their program
speak because they need to find some suitable synthesiser (long hours of
experiments and so on...) and then make it work with their program.  They
often need to write output device drivers for these programs or hardware
devices and are doing it again and again.  You can imagine it all fails
when an innocent user executes two programs with speech output at once --
if they even start both (what I doubt), they will be shouting one over the
other.  This makes it very hard for programmers to implement speech support
to their programs (for blind users or simply to make a better user
interface) and it's one of the reasons we still don't fully exploit what
speech synthesis technology offers.

In an ideal world, programmers could use similar commands for speech
synthesis as they do for normal text output (printf, puts, ...).  In an
ideal world, there would be some speech_printf() that would take care of
saying your message in the right time without interumping others, without
you beeing obligated to take care of how exactly the communication with
speech synthesiser is implemented and without you having to worry about
what synthesiser to use and if it's available.  In an ideal world, there
would be some speech synthesiser in each GNU/Linux distribution and some
speech deamon taking care of all applications that want to speak,
allowing user to configure speech parameters and providing simple interface
(as speech_printf()) through some shared library for programmers.  It will
be a long way until we get archieve this state of things, but with Speech
Daemon, we are taking the first steps...

@node Current state, Basic design, Why and how, Introduction
@section Current state
@cindex Synthesizers
@cindex Other programs

Today, the development of programs and new technologies connected
with speech synthesis under GNU/Linux is centered around two main
points: visually impaired people and pure development. Although some
fields are beginning to use synthesis for different purposes, like
telephony servers, these are still like drops of water in the ocean.
Here is a short (definitely not exhaustive list) software synthesizers,
hardware synthesizers and applications known to work under GNU/Linux.

@enumerate
@item Speech Synthesizers

@itemize @bullet

@item Hardware synthesizers

Hardware synthesizers are the devices, which may be connected to PC.
Mostly they are external, connected via serial or parallel port.  There
are also some internal devices for ISA bus or USB.  Application may
send textual data to the port and the device converts it to spoken
letters and words.  Data may contain also several control sequences in
the form of escaped characters as commands.  The problem, we are
facing, is that each of these devices uses its own communication
protocol.

@item Software synthesizers
@pindex Festival
@pindex Flite
@pindex Odmluva
@pindex Epos
@pindex FreeTTS

@itemize @minus

@item Festival

Festival is a multilingual Free Software text to speech synthesiser
with high quality speech databases available. One of it's problems
is that some of the most important databases are not free.
(eg. the database for British English is non-free). Other problem
is that Festival is intended rather as a platform for research
and developement than as an end-user product and therefore is
big and not-so-easy to install. The problem we face as Speech Deamon
Developers is that it's too slow to be really useful for most
aplications.

@item Flite

Flite stands for Festival Lite and it is a light fully free english
speech software synthesiser with good quality of sound, developed
by the authors of festival as an end-user product. It's very fast,
however, we currently don't know how to configure it (it seems it
is not possible yet) and it seems that the developers have some
problems with importing the voices from Festival. Speech Deamon
currently uses Flite as it's primary output module for English.

@item Odmluva

Odmluva is a simple (and very light) czech speech synthesizer available
under the terms of GNU GPL. We are working on it's support in Speech Deamon.

@item Epos

Epos is Czech synthesis. It is an academical project and it
already gives quite good results, but some parts are covered
by a proprietary license.
@c TODO: We need to add more information.

@item Free TTS

Free TTS is some JAVA-based text-to-speech system. We didn't
checked it yet.
@c TODO: check it...

@item IBM ViaVoice

ViaVoice is a multi-lingual software synthesizer available for GNU/Linux.
The main problem is that ViaVoice is not free (as in freedom). Until IBM
changes its licence, we can't use it in Free World / Free Operating System
and therefore it's not and will not be supported in Speech Deamon.

@item MBROLA

MBROLA is a multi-lingual software synthesis available for GNU/Linux.
MBROLA is not free as in freedom, although it's gratis. The same problems
as with IBM ViaVoice prevents us to include it in Speech Deamon.

@end itemize
@end itemize

@item Speaking applications

@itemize @minus

@item EmacSpeak

The EmacSpeak (by T. V. Raman <@email{raman@@cs.cornell.edu}>) software package
provides speech output for Emacs, and includes ,,speech servers'' for
the DECtalk speech synthesizers.

The package emacspeak-ss provides servers for several additional synthesizers.
None of these programs are normally run by the user directly.  Instead,
they are run by Emacs. That is: Emacs runs the emacspeak code, which executes
Tcl, which interprets the server code. This approach is too closely ,,wired''
to usage with Emacspeak, so it can't be used for our general purposes.

This does not mean, that these servers are compleetly a bad idea and we
can not use them. Thanks to the author Jim Van Zandt <@email{jrv@@vanzandt.mv.com}>,
we can learn from the sources and write the output driver modules for Speech Daemon
(emacspeek-ss is GPL).

@item GTK+ (Gnome Accessibility project)

GNOME windowing toolkit library.

@item wxWindows

Windowing toolkit library.

@item Java AWT

Windowing toolkit library.

@item FOX toolkit

Windowing toolkit library.

@item Speakup

Speakup is a kernel patch that provides low level speech output for visually
impaired, so it works even if there is some problem in configuration and
you can't run EmacSpeak.

@item Brltty

Brltty is mainly a driver for different Braille displays, but also supports
some kind of software synthesis.

@end itemize
@end enumerate

We hope to be able to integrate Speech Daemon into these projects
in the future.

@node   Basic design, User, Current state, Introduction
@section Design
@cindex Design

The communication between all
these applications and synthesizers is a great mess. For this purpose,
we wanted Speech Deamon to be a layer separating applications and
synthesizers so that apps wouldn't have to care about synthesizers
and synthesizers wouldn't have to care about interaction with apps.

We decided we would implement Speech Deamon as a server receiving
commands from applications over a protocol called @code{SSIP},
parsing them and if it's necessary and calling appropiate functions
of output modules communicating with the different synthesizers.
These output modules are implemented as plug-ins, so that the user
can just load a new module if he wants to use new synthesizer.

Each client (application that wants to speaks) opens a socket
connection to Speech Deamon and calls functions like spd_say(),
spd_stop(), spd_pause() provided by the shared library. This
shared library is still on the client side and sends Speech
Deamon SSIP commands over the socket. When these arrive
at Speech Deamon, it parses them, reads the text that should
be said and put it in a queues according to the priority
of this message and other criterions. It then decides when,
with which parameters (set up by the client and the user)
and on which synthesizer it will say the message. These requests
are handled by the output plug-ins (output modules) for different
hardware and software synthesizers and then said aloud.

See this figure:

@image{figures/architecture,,,Speech Deamon architecture (you can see a text
version of this figure in the Info manual)}

See also the detailed description of SSIP, public API and module API.

@node User, Programmer, Basic design, Introduction
@section User's point of view

In this section we will try to describe what can Speech Deamon offer
to common users. But every programmer interested in this program should
also read this because it's very important to understand.

@itemize @bullet
Sketch:
@item easy configuration of different speaking apps, central maintaince
@item the ability to freely choose which synthetizer with which app
@item less time devoted to configuration and tuning different apps and synthesis
@item history of said messages for visually impaired
@end itemize

@c TODO: needs a lot of more work

@node Programmer,  , User, Introduction
@section Programmer's point of view

@itemize @bullet
Sketch:
@item easy way to make your apps speak
@item no time spent on configuration/debugging interface with different synthesizers
@item no need to take care about configuration of voice
@item easy way to make the app accessible to visually impaired people
@item different facilities like the one providing a command line functionality
@end itemize

@node Invoking, Internal structure, Introduction, Top
@chapter Invoking


@menu
* Verbosity::                   Definition of the different verbosity levels.
@end menu

@node Verbosity,  , Invoking, Invoking
@section Verbosity

There are 6 different verbosity levels of Speech Deamon logging.
0 means there is no output, while 5 means that nearly all the information
about Speech Deamon working is written to stdout.

@subsection Level 0
No information.

@subsection Level 1
@itemize @bullet
@item Information about loading and exiting.
@end itemize

@subsection Level 2
@itemize @bullet
@item Information about errors that ocured.
@item Allocating and freeing resources on start and exit.
@end itemize

@subsection Level 3
@itemize @bullet
@item Information about accepting/rejecting/closing clients' connections.
@item Information about invalid client commands.
@end itemize

@subsection Level 4
@itemize @bullet
@item Every received command is output.
@item Information about proceeding the command output
@item Information about queueing/allocating messages.
@item Information about the function of history, sound icons and other
facilities.
@item Information about the work of the speak() thread.
@end itemize

@subsection Level 5
This is only for debugging purposes and can output really *much*
data. Use with caution.
@itemize @bullet
@item Also received data (messages etc.) is output.
@end itemize

@node Internal structure, Public API, Invoking, Top
@chapter Internal structure

@menu
* Definitions::                 What is output module, who is client...
* Server core::                 Message handling, configuration, history
* Output modules::              How they work and what we need from them
@end menu

@node Definitions, Server core, Internal structure, Internal structure
@section Definitions

@dfn{Server side} is the side where Speech Deamon operates. It
means server core, output modules and partly SSIP which is the layer
for communication between server side and client side.

@dfn{Client side} is where particular applications wanting to speak
are, where the shared library implementing public API is
located and partly SSIP which is the layer
for communication between server side and client side.

@dfn{Client} means an application that wants to speak or an application
that is used to control Speech Deamon. (Of course different combinations
are possible.)

@dfn{Server core} is the central part of Speech Deamon composed of
two threads. One is listening on the user socket, parsing and proceeding
incommimg commands, and saving incomming text to queues. The other thread
takes messages from queues and sends them to appropiate synthesizers.

@dfn{Output module} is a backend of Speech Deamon in the form of plug-in.
It takes care of communication with the particular synthesizer and provides
only abstract functions to the server core.

@dfn{Shared library} or @dfn{Public API} is a front-end of Speech Deamon
that provides polished functions programmers should
use to send commands to the server.

@dfn{SSIP} is the layer (communication protocol) between server side
(server core) and client side (shared library). It stands for Speech
Synthesis Internet Protocol.  Client programs should never use it
directly.

@dfn{Socket} or @dfn{File descriptor} represents the particular connection
between a client and server. In C, it's and integer variable.

@node Server core, Output modules, Definitions, Internal structure
@section Server core

see sources, I'll try to write this section soon

@node Output modules,  , Server core, Internal structure
@section Output modules

@findex synthesizer_write()
@findex synthesizer_stop()
@findex synthesizer_is_speaking()
@findex synthesizer_close()

Output modules for Speech Deamon have the form of a glib plug-ins
located in src/modules/. Each output module is a data
structure composed of some parameters and pointers to it's functions.

@example
typedef struct @{
  gchar    *name;
  gchar    *description;
  gchar    *filename;
  gint     (*write)    (const gchar *, gint, void*);
  gint     (*stop)     (void);
  gint     (*is_speaking) (void);
  gint     (*close)    (void);
@} OutputModule;
@end example

This structure is defined in @file{intl/modules.h} and therefore 
this header must be included in every plugin source code.

@example
#include "modules.h"
@end example

Also one other file called @file{intl/fdset.h} where the FDSetElement
structure is defined must be included to be able to handle the
different speech synthesis settings.

@example
#include "fdset.h"
@end example

Each output module has associated a module_init function
that is called at the starting of Speech Deamon. After doing
the necessary initialization, it must return a filled structure
of the type OutputModule (defined above).

@example
OutputModule *module_init(void)@{
        ...
        return &module_flite;
@}
@end example

Now what are the 4 functions: flite_write, flite_stop, flite_is_speaking
and flite_close? This is the core of every output module and you
have to define their bodies in the source code of your plug-in.

@deffn {Output module functions}  gint synthesizer_write const gchar *data, gint len, TFDSetElement* set

This is the function where actual speech output is produced. It is called
every time Speech Deamon decides to send a message to synthesis. The data
of lenght @var{len} are passed in @var{data}. Additionally, the structure
containing settings associated to this particular message is passed,
however only few options are important for output modules.

Each output module should take care of setting the output device to these parameters
(the other ones are handled independently in other parts of Speech Deamon):
@itemize @bullet
@item (signed int) set->speed
@item (signed int) set->pitch
@item (char*) set->language
@item (int) set->voice_type
@end itemize

Speed and pitch are values between -100 and 100 included. 0 is the default
value that represents normal speech flow. So -100 is the slowest (or lowest)
and +100 is the fastest (or highest) speech.

(We should estabilish a constant scale refered to some text, standard speeds
and different associated times. This will probably be the work of the person
who will program the first real output module. We need to chose some longer
text, decide what speed of reading we consider 0 and what we consider, say,
+-50, measure the times needed to read it at these speeds and put it there
in documentation as our standard scale.)

The language parameter is given as a null-terminated string containnig 
the name of the language in english in lowercase (e.g. "english", "czech", "spanish").

voice_type is used only when the output module supports more types of voices
for this particular language. The values represent (from @file{intl/fdset.h})
@example
typedef enum @{
    MALE = 0,
    FEMALE = 1,
    CHILD_MALE = 2,
    CHILD_FEMALE = 3
@}EVoiceType;
@end example
We can consider also other voice types. 

This function should return 0 if it fails and some non-0 value
if the delivery to the synthesis is succesful. Formerly we thought
that it should return the number of bytes written, but it's still
not clear how to handle messages that have to be divided in more
parts (for example if the output device has a finite size buffer).

@end deffn

@deffn {Output module functions}  {gint synthesizer_stop} void

This function should stop the synthesis of the currently spoken message
immediately and throw away the rest of the message.

It should return 0 on succes, -1 otherwise.

@end deffn

@deffn {Output module functions}  {gint synthesizer_is_speaking} void

This function is very important to let Speech Deamon know how to
regulate the speech flow between different queues, programs and even
other synthesizers. On calling it, the output module must decide
whether there is currently any output beeing produced in the speakers.

This can be a very hard problem and it's not clear how to do it
with diferent synthesizers. If it's not possible to return an exact
value, at least some estimate should be calculated. But such an inacurate
value can highly reduce the usefulness of an even otherwise very good
plug-in. To some degree, this is still an open question.

It should return 0 if the synthesis is silent, 1 if it's speaking.

@end deffn

@deffn {Output module functions}  {gint synthesizer_close} void

This function is called when Speech Deamon terminates. There are no
special requierements on what the output plug-in should do.

It should return 0 on succes, -1 otherwise.

@end deffn

@node Public API, SSIP, Internal structure, Top
@chapter Public API
@findex spd_init()
@findex spd_close()
@findex spd_say()
@findex spd_sayf()
@findex spd_stop()
@findex int spd_pause()
@findex int spd_resume()
@findex spd_command_line()
@findex spd_stop_fd()
@findex spd_pause_fd()
@findex spd_resume_fd()
@findex spd_history_select_client()
@findex spd_get_client_list()
@findex spd_get_message_list_fd()

@ifinfo
@verbatiminclude ../src/clients/libspeechd.h
@end ifinfo

@c don't know if it's really The Right Thing, but I often miss *any*
@c description in the header files, and there is literate programming
@c on the other side. if you have a reason why it's not ok, I'm open
@c to the idea to move it all here

@node   SSIP, Priorities, Public API, Top
@chapter SSIP
Shared library implementimg public API like
@code{spd_sayf(int, int, char*, ...)} communicates
with Speech Deamon through SSIP, Speech Synthesis Internet
Protocol.

Here is a description of it's commands and what they do.

@menu
* Data flow and speech control::  
* Settings::                    
* Message history handling::    
* Return values::               
@end menu

@node Data flow and speech control, Settings, SSIP, SSIP
@section Data flow and speech control

Note that some "less important" aspects of SSIP, e.g. history commands,
can still change in near future.

@deffn {SSIP Command} {SPEAK}
As Speech Deamon receives this command, it becomes listening for
incomming data. It adds incomming data to it's buffer until 
@w{@code{LFCR.LFCR}} comes. There is no upper limit on the amount
of data sent, but be aware that the user can set Speech Deamon
to ignore parts of very long messages from some number of (kilo)bytes.

When @code{LFCR.LFCR}, the data flow is terminated and Speech
Deamon switches to normal command mode. If you want to send
this sequence without being interpreted as command, you have
to change it to @code{LFCR..LFCR}, Speech Deamon will change
that back to @code{LFCR.LFCR} inside. If your program is
sending user's input, be sure to check for this combination
inside the text before sending it!
@end deffn

@deffn {SSIP Command} {STOP} [uid|"all"]
Immediately stop speaking from this client and throw away all messages.

If it has no parameter, it's related only to that client.

If there is a positive integer number @code{uid} as parameter,
it's related to some other client with unique id @code{uid}.

If the parameter is the string all, it stops all clients
whatsoever and throws away all messages whatsoever.
 
@end deffn

@deffn {SSIP Command} {PAUSE} [uid]
Immediately stop speaking and pause all messages.

If it has no parameter, it's related only to that client.

If there is a positive integer number @code{uid} as parameter,
it's related to some other client with unique id @code{uid}.
@end deffn

@deffn {SSIP Command} {RESUME} [uid]
Start speaking again and say all unpaused messages.
(Note that messages level 3 are never said after pause).

If it has no parameter, it's related only to that client.

If there is a positive integer number @code{uid} as parameter,
it's related to some other client with unique id @code{uid}.
@end deffn

@node Settings, Message history handling, Data flow and speech control, SSIP
@section Settings

@deffn {SSIP Command} {SET CLIENT_NAME} str
Set the name of this client to @code{str}. String parameter @code{str}
must be in the format @w{@code{client_name:subclient_name}}.

For example if the client is lynx and it creates connection for it's
command line, the command would be something like this:
@example
SET CLIENT_NAME lynx:cmd_line
@end example
@end deffn

@deffn {SSIP Command} {SET LANGUAGE} str
Set recommended language for this client to @code{str}.
The @code{str} parameter is the language name.

For example, if we want to set the prefered language to Czech,
we can send:
@example
SET LANGUAGE czech
@end example
@end deffn

@deffn {SSIP Command} {SET PRIORITY} num
Set the priority for this client to @code{num}.
It must be either 1, 2 or 3. 1 is the highest, 3 the lowest.
@end deffn

@node Message history handling, Return values, Settings, SSIP
@section Message history handling

@deffn {SSIP Command} @w{HISTORY GET CLIENT_LIST}
Retrieves a list of know client names, its unique ids and
if they are gone or alive in the form:

@code{240-uid client_name:subclient_name active}
@code{240 OK CLIENTS LIST SENT}

The reply can look for example like this:
@example
240-0 speechd_client:main 0
240-1 speechd_client:status 0
240-2 unknown:unknown 1
240 OK CLIENTS LIST SENT
@end example
@end deffn

@deffn {SSIP Command} {HISTORY GET CLIENT_MESSAGES} client_uid from num
Retrieves @code{num} messages from the buffer of specified client (@code{client_uid}),
starting from the number in parameter @code{from}, in the follwing form:

@example
241-uid client_name:subclient_name
241 OK MSGS LIST SENT
@end example

Warning: This is going to be subject to future changes!
@end deffn

@deffn {SSIP Command} @w{HISTORY GET LAST}
Get the uid of last received message. Useful if the user
wants to repeat something he didn't hear.

The reply is in the form:
@example
242-uid client_name:subclient_name
242 OK LAST MSG SAID
@end example

@end deffn

@deffn {SSIP Command} @w{HISTORY SORT} ...
Sorts the history buffer of messages.

Well, this is not implemented yet. I'm sorry...
@end deffn

@deffn {SSIP Command} {HISTORY CURSOR SET} uid ["FIRST","LAST","POS" position]

Sets the position of cursor in the history to some particular place.
Note that the buffer can be sorted, see @code{HISTORY SORT}.

If the parameter is @code{FIRST} or @code{LAST}, Speech Deamon sets
the position to the first or to the last message of client @code{uid}
respectively.

It the parameter is @code{POS}, an aditional integer parameter
@code{position} must be specified. Speech Deamon then sets the position
of cursor to the message number @code{pos} of client @code{uid}.
@end deffn

@deffn {SSIP Command} {HISTORY CURSOR} ["NEXT"|"PREV"]
Move the cursor one position forward or backward between the messages
of currently specified client.
@end deffn

@deffn {SSIP Command} @w{HISTORY CURSOR GET}
Get the unique id number of the message the cursor is pointing on.

The reply is in the form:
@example
243-uid
243 OK CURSOR POSITION RETURNED
@end example

This is still going to change!

@end deffn

@deffn {SSIP Command} {HISTORY SAY ID} id
Send message ID to Speech Deamon core to say it aloud.
@end deffn

@deffn {SSIP Command} @w{HISTORY SAY TEXT} str
Say text @code{str} without including it in history. Useful for
history clients. Not implemented yet...
@end deffn

@node Return values,  , Message history handling, SSIP
@section Return values

@multitable {codes} {name of group} {more detailed information about the command and what it does}
@item 1xx    @tab     META          @tab   information about the protocol, help
@item 2xx    @tab     OK            @tab   ok, specification of the performed action
@item 3xx    @tab     SERVER ERROR  @tab   problems on server side
@item 4xx    @tab     CLIENT ERROR  @tab   problems on client side, requests that don't make sense
@item 5xx    @tab     SYNTAX ERROR  @tab   errors in syntax of SSIP commands
@item 6xx    @tab     reserved
@end multitable 

Please see @file{src/server/msg.h} for detailed description.

@node Priorities, Multiple output modules, SSIP, Top
@chapter Priorities
@cindex priorities

The possibility to distinguish between several message priority levels
seems to be essential. Each message sent by client to speech server
should have a priority level assigned.

Speech Deamon provides the system of three priority levels. Every message will
either contain explicit level information, or the default value will be
considered. There is a separate message queue for each of the levels.
The behavior is as follows:

@section Level 1
These messages will be said immediately as they come to server.
They are never interrupted. These messages should be as short
as possible, because they block the output of all other
messages. When several concurrent messages are received by
server, they are queued and said in the order, they came.
When a new message of level 1 comes during lower level
message is spoken, lower level message is canceled and removed
from the queue (this message is allready stored in the history)

@section Level 2
Second level messages are said in the moment, when there is no
message of level 1 queued. Several messages of level 2 are said
in the order, they are received (queued, but in their own
queue). This is the default level for messages without explicit
level information.

@section Level 3
Third level messages are only said, when there are no messages
of any higher level queued. If there are level 3 messages beeing
said or waiting in queues, they are interrupted by the last
incomming level 3 message and this one is said, in other words,
level 3 is interrupting itself.

@section How to use them wisely

Example uses for level @strong{one} are:
 
@itemize
@item error messages
@item very important messages
@item ...
@end itemize

Example uses for level @strong{two} are:

@itemize
@item regular program messages
@item menus
@item text the user is working on
@item ...
@end itemize

Example uses for level @strong{three} are:

@itemize
@item less important status information
@item letters when typing input
@item ...
@end itemize


@node Multiple output modules, Messsage history, Priorities, Top
@chapter Multiple output modules
@cindex output module
@cindex different synthesizers

Speech Deamon supports concurrent use of multiple output modules.
In the case these output modules provide good synchronization,
you can combine them in reading messages. For example if module1 can
speak English and Czech while module2 speaks only German, the idea
is that if there is something message in German, module2 is used,
while module1 is used for the other languages. These rules for
selection of output modules can be influenced through the configuration
file @file{speechd.conf}.

If you want to compile and use a new output module, you should place
it in @file{src/modules} in your source directory of Speech Deamon and
add it to @file{src/modules/Makefile.am}. You can compile and install
it by typing: @code{make; su root; make install}. The last step you
have to do is to let Speech Deamon know you want to use this new
module by adding a line to @file{speechd.conf} in your configuration directory
@example
AddModule module_name
@end example
and possibly also changing the line
@example
DefaultModule new_module
@end example
to make it default.

@xref{Output modules}.

@node Messsage history, Speech parameters, Multiple output modules, Top
@chapter Message history

@node Speech parameters, Configuration, Messsage history, Top
@chapter Speech parameters
@cindex Speech parameters
@cindex Settings

@section Language selection

Various synthesizers provide different sets of possible
languages, they are allowed to speak. We must be able to
receive a request for setting particular language (using
ISO language code) and reply, if the language is supported.

@section Speed

Sped of the speech is supported by all synthesizers, but the
values and their ranges differ. Each output module is
responsible to set the speed to the value, best responding to
current setting. This may be a little bit difficult, because
there is no exact scale. We could take some longer english
paragraph and take it as a base for our new scale. If this
paragraph is said in eg. ten secconds, this means speed = 100,
if it is said in twenty seconds, speed = 200. This way, we
can coordinate  diferent scales quite preciselly (the paragraph
should be long enough).

@section Punctuation mode

Punctuation mode describes the way, in which the synthesizer
works with non-alphanumeric characters. Most synthesizers
support several punctuation modes. We will support a reasonable
superset of those modes, which may be implemented in device
driver, when not supported by hardware.

@section Prosody
Prosody setting allows us, to distinguish interpunction
characters in spoken text, as we are familiar in normal speech.
This means the way, we pronounce the text with interrogation mark,
coma, dot etc.

@section Pitch
Pitch is the voice frequency. We face the similar problems
here, as with Speed setting.

@section Voice type
Most synthesizers provide several voice types, such as male,
female, child etc. The set is again different for each
of the devices. Speech Daemon should try to find the nearest
possible (if the request is child female and it's not available,
we will try to use adult female rather then adult male).

@section Spelling
Spelling mode is provided by nearly all devices and is also
easy to emulate in output module.

@section Capital letters recognition
That is again a widely supported feature. However it is
desirable to support this internally, using the
sound icons feature, but this  requires a good possibility of
synchronization, which is not  possible with all devices.


@node Configuration, Copying, Speech parameters, Top
@chapter Configuration
@cindex configuration
@cindex default values

Speech Deamon can be configured on several levels.
There is a configuration file where permanent settings
are stored, but user and applications can also change
the majority of parameters on-fly by calling Speech Deamon
functions. The third level of configuration can't be
changed and it's given by the capabilities of each output
device (each output module for each output device reports
it's capabilities when it's loaded into Speech Deamon).

We use DotConf for the permanent textfile-based configuration.
See @file{speechd.conf}.

Other parts of this manual deal with the runtime configuration.


@node Copying, Concept index, Configuration, Top
@appendix GNU GENERAL PUBLIC LICENSE
@center Version 2, June 1991

@include gpl.texi


@node Concept index,  , Copying, Top
@unnumbered Concept index

@cindex tail recursion
@printindex cp

@contents
@bye

@c speechd.texi ends here

