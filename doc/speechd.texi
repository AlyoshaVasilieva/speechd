\input texinfo   @c -*-texinfo-*-
@c %**start of header
@setfilename speechd.info
@settitle Speech Daemon --- Easy acces to speech synthesis
@finalout
@c @setchapternewpage odd
@c %**end of header

@syncodeindex pg cp
@syncodeindex fn cp
@syncodeindex vr cp

@include version.texi
     
@copying
This manual is for Speech Daemon, version @value{VERSION}.

Copyright @copyright{} 2001, 2002, 2003 ???

@quotation
Permission is granted to ...
@end quotation
@end copying

@dircategory Sound
@dircategory Development

@direntry
* Speech Daemon: (speechd).       Speech Daemon.
@end direntry


@titlepage
@title Speech Daemon --- Easy acces to speech synthesis
@subtitle Mastering the Babylon of TTS'
@subtitle Edition @value{EDITION}, for Speech Daemon @value{VERSION}
@subtitle @value{UPDATED}
@author by Tomas Cerha <@email{cerha@@brailcom.cz}>, Hynek Hanke <@email{hanke@@volny.cz}>
@author and Czech Free Software organization <@email{info@@freesoft.cz}>

@page
@vskip 0pt plus 1filll

@insertcopying

@end titlepage

@c So the toc is printed in the right place.
@contents

@ifnottex
@node Top, Instructions, (dir), (dir)

This file documents the @code{speechd} client/server application 
that attempts to provide a common interface to different synthesizers.

@insertcopying
@end ifnottex

@menu
* Instructions::             How to read this manual.
* Introduction::             What is Speech Daemon.
* Invoking::                 How to start Speech Daemon.
* Internal structure::       How does Speech Daemon work.
* Public API::               How to use Speech Daemon in your programs.
* SSIP::                     Speech Synthesis Internet Protocol.
* Priorities::               Description, guidelines how to use them.
* Multiple output modules::  Using different synthesizers.
* Messsage history::         Handling history of arrived messages.
* Speech parameters::        Settings that affect the way Speech Daemon speaks.
* Configuration::            How to configure Speech Daemon.

* Standard sound icons::     List of the standard sound icon names.
* Copying::                  GNU General Public License.

* Concept index::            Index of concepts.
@end menu

@node Instructions, Introduction, Top, Top
@chapter How to read this manual
@cindex How to read

-> there should be a simple map of the manual and where to search for different concepts

@node Introduction, Invoking, Instructions, Top
@chapter Introduction

@menu
* Why and how::                 Why Speech Daemon?  Philosophy, motivation...
* Current state::               Speaking software today.
* Basic design::                How does it work?
* User::                        Speech Daemon from user's point of view.
* Programmer::                  Speech Deam from programmer's poing of view.
@end menu

@node Why and how, Current state, Introduction, Introduction
@section Why and how
@cindex Basic ideas, Motivation
@cindex Philosophy

Speech Daemon project comes to provide a device independent layer for
speech synthesis. It should provide a simple interface for client
applications (applications, that want to speak) as well as for device
driver modules (for particular speech synthesis).

High quality speech synthesis has been available for a long time and now
it's usable even by ordinary users on their home PC's. It comes sometimes
as a necesity, sometimes as a good feature for programs to provide speech
output.  There is a wide field of possible uses from educational software,
through specialized systems (hospitals, laboratories, telephony servers).
For visually impaired users it is one of the two essential ways of getting
the output from computer (the second one is Braille display). That's also
where Speech Daemon comes from.

There are different speech synthesisers with different capabilities.  Some
of them are hardware, some of them are software.  Some of them are Free
Software and are are available on the Internet.  However, none of them is
preinstalled in one of the widely used GNU/Linux distributions.
Programmers have really hard times when they want to make their program
speak because they need to find some suitable synthesiser (long hours of
experiments and so on...) and then make it work with their program.  They
often need to write output device drivers for these programs or hardware
devices and are doing it again and again.  You can imagine it all fails
when an innocent user executes two programs with speech output at once ---
if they even start both (what I doubt), they will be shouting one over the
other.  This makes it very hard for programmers to implement speech support
to their programs (for blind users or simply to make a better user
interface) and it's one of the reasons we still don't fully exploit what
speech synthesis technology offers.

In an ideal world, programmers could use similar commands for speech
synthesis as they do for normal text output (printf, puts, ...).  In an
ideal world, there would be some speech_printf() that would take care of
saying your message in the right time without interumping others, without
you beeing obligated to take care of how exactly the communication with
speech synthesiser is implemented and without you having to worry about
what synthesiser to use and if it's available.  In an ideal world, there
would be some speech synthesiser in each GNU/Linux distribution and some
speech daemon taking care of all applications that want to speak,
allowing user to configure speech parameters and providing simple interface
(as speech_printf()) through some shared library for programmers.  It will
be a long way until we get archieve this state of things, but with Speech
Daemon, we are taking the first steps...

@node Current state, Basic design, Why and how, Introduction
@section Current state
@cindex Synthesizers
@cindex Other programs

Today, the development of programs and new technologies connected
with speech synthesis under GNU/Linux is centered around two main
points: visually impaired people and pure development. Although some
fields are beginning to use synthesis for different purposes, like
telephony servers, these are still like drops of water in the ocean.
Here is a short (definitely not exhaustive list) software synthesizers,
hardware synthesizers and applications known to work under GNU/Linux.

@enumerate
@item Speech Synthesizers

@itemize @bullet

@item Hardware synthesizers

Hardware synthesizers are the devices, which may be connected to PC.
Mostly they are external, connected via serial or parallel port.  There
are also some internal devices for ISA bus or USB.  Application may
send textual data to the port and the device converts it to spoken
letters and words.  Data may contain also several control sequences in
the form of escaped characters as commands.  The problem, we are
facing, is that each of these devices uses its own communication
protocol.

@item Software synthesizers
@pindex Festival
@pindex Flite
@pindex Odmluva
@pindex Epos
@pindex FreeTTS

@itemize @minus

@item Festival

Festival is a multilingual Free Software text to speech synthesiser
with high quality speech databases available. One of it's problems
is that some of the most important databases are not free.
(eg. the database for British English is non-free). Other problem
is that Festival is intended rather as a platform for research
and developement than as an end-user product and therefore is
big and not-so-easy to install. The problem we face as Speech Daemon
Developers is that it's too slow to be really useful for most
aplications.

@item Flite

Flite stands for Festival Lite and it is a light fully free english
speech software synthesiser with good quality of sound, developed
by the authors of festival as an end-user product. It's very fast,
however, we currently don't know how to configure it (it seems it
is not possible yet) and it seems that the developers have some
problems with importing the voices from Festival. Speech Daemon
currently uses Flite as it's primary output module for English.

@item Odmluva

Odmluva is a simple (and very light) czech speech synthesizer available
under the terms of GNU GPL. We are working on it's support in Speech Daemon.

@item Epos

Epos is Czech synthesis. It is an academical project and it
already gives quite good results, but some parts are covered
by a proprietary license.
@c TODO: We need to add more information.

@item Free TTS

Free TTS is some JAVA-based text-to-speech system. We didn't
checked it yet.
@c TODO: check it...

@item IBM ViaVoice

ViaVoice is a multi-lingual software synthesizer available for GNU/Linux.
The main problem is that ViaVoice is not free (as in freedom). Until IBM
changes its licence, we can't use it in Free World / Free Operating System
and therefore it's not and will not be supported in Speech Daemon.

@item MBROLA

MBROLA is a multi-lingual software synthesis available for GNU/Linux.
MBROLA is not free as in freedom, although it's gratis. The same problems
as with IBM ViaVoice prevents us to include it in Speech Daemon.

@end itemize
@end itemize

@item Speaking applications

@itemize @minus

@item Emacspeak

The Emacspeak (by T. V. Raman <@email{raman@@cs.cornell.edu}>) software package
provides speech output for Emacs, and includes ,,speech servers'' for
the DECtalk speech synthesizers.

The package emacspeak-ss provides servers for several additional synthesizers.
None of these programs are normally run by the user directly.  Instead,
they are run by Emacs. That is: Emacs runs the emacspeak code, which executes
Tcl, which interprets the server code. This approach is too closely ,,wired''
to usage with Emacspeak, so it can't be used for our general purposes.

This does not mean, that these servers are compleetly a bad idea and we
can not use them. Thanks to the author Jim Van Zandt <@email{jrv@@vanzandt.mv.com}>,
we can learn from the sources and write the output driver modules for Speech Daemon
(emacspeek-ss is GPL).

@item GTK+ (Gnome Accessibility project)

GNOME windowing toolkit library.

@item wxWindows

Windowing toolkit library.

@item Java AWT

Windowing toolkit library.

@item FOX toolkit

Windowing toolkit library.

@item Speakup

Speakup is a kernel patch that provides low level speech output for visually
impaired, so it works even if there is some problem in configuration and
you can't run Emacspeak.

@item Brltty

Brltty is mainly a driver for different Braille displays, but also supports
some kind of software synthesis.

@end itemize
@end enumerate

We hope to be able to integrate Speech Daemon into these projects
in the future.

@node Basic design, User, Current state, Introduction
@section Design
@cindex Design

The communication between all
these applications and synthesizers is a great mess. For this purpose,
we wanted Speech Daemon to be a layer separating applications and
synthesizers so that apps wouldn't have to care about synthesizers
and synthesizers wouldn't have to care about interaction with apps.

We decided we would implement Speech Daemon as a server receiving
commands from applications over a protocol called @code{SSIP},
parsing them and if it's necessary and calling appropiate functions
of output modules communicating with the different synthesizers.
These output modules are implemented as plug-ins, so that the user
can just load a new module if he wants to use new synthesizer.

Each client (application that wants to speaks) opens a socket
connection to Speech Daemon and calls functions like spd_say(),
spd_stop(), spd_pause() provided by the shared library. This
shared library is still on the client side and sends Speech
Daemon SSIP commands over the socket. When these arrive
at Speech Daemon, it parses them, reads the text that should
be said and put it in a queues according to the priority
of this message and other criterions. It then decides when,
with which parameters (set up by the client and the user)
and on which synthesizer it will say the message. These requests
are handled by the output plug-ins (output modules) for different
hardware and software synthesizers and then said aloud.

See this figure:

@image{figures/architecture,,,Speech Daemon architecture (you can see a text
version of this figure in the Info manual)}

See also the detailed description of SSIP, public API and module API.

@node User, Programmer, Basic design, Introduction
@section User's point of view

In this section we will try to describe what can Speech Daemon offer
to common users. But every programmer interested in this program should
also read this because it's very important to understand.

@itemize @bullet
Sketch:
@item easy configuration of different speaking apps, central maintaince
@item the ability to freely choose which synthetizer with which app
@item less time devoted to configuration and tuning different apps and synthesis
@item history of said messages for visually impaired
@end itemize

@c TODO: needs a lot of more work

@node Programmer,  , User, Introduction
@section Programmer's point of view

@itemize @bullet
Sketch:
@item easy way to make your apps speak
@item no time spent on configuration/debugging interface with different synthesizers
@item no need to take care about configuration of voice
@item easy way to make the app accessible to visually impaired people
@item different facilities like the one providing a command line functionality
@end itemize

@node Invoking, Internal structure, Introduction, Top
@chapter Invoking


@menu
* Verbosity::                   Definition of the different verbosity levels.
@end menu

@node Verbosity,  , Invoking, Invoking
@section Verbosity

There are 6 different verbosity levels of Speech Daemon logging.
0 means there is no output, while 5 means that nearly all the information
about Speech Daemon working is written to stdout.

@subsection Level 0
No information.

@subsection Level 1
@itemize @bullet
@item Information about loading and exiting.
@end itemize

@subsection Level 2
@itemize @bullet
@item Information about errors that ocured.
@item Allocating and freeing resources on start and exit.
@end itemize

@subsection Level 3
@itemize @bullet
@item Information about accepting/rejecting/closing clients' connections.
@item Information about invalid client commands.
@end itemize

@subsection Level 4
@itemize @bullet
@item Every received command is output.
@item Information about proceeding the command output
@item Information about queueing/allocating messages.
@item Information about the function of history, sound icons and other
facilities.
@item Information about the work of the speak() thread.
@end itemize

@subsection Level 5
This is only for debugging purposes and can output really *much*
data. Use with caution.
@itemize @bullet
@item Also received data (messages etc.) is output.
@end itemize

@node Internal structure, Public API, Invoking, Top
@chapter Internal structure

@menu
* Definitions::                 What is output module, who is client...
* Server core::                 Message handling, configuration, history
* Output modules::              How they work and what we need from them
@end menu

@node Definitions, Server core, Internal structure, Internal structure
@section Definitions

@dfn{Server side} is the side where Speech Daemon operates. It
means server core, output modules and partly SSIP which is the layer
for communication between server side and client side.

@dfn{Client side} is where particular applications wanting to speak
are, where the shared library implementing public API is
located and partly SSIP which is the layer
for communication between server side and client side.

@dfn{Client} means an application that wants to speak or an application
that is used to control Speech Daemon. (Of course different combinations
are possible.)

@dfn{Server core} is the central part of Speech Daemon composed of
two threads. One is listening on the user socket, parsing and proceeding
incommimg commands, and saving incomming text to queues. The other thread
takes messages from queues and sends them to appropiate synthesizers.

@dfn{Output module} is a backend of Speech Daemon in the form of plug-in.
It takes care of communication with the particular synthesizer and provides
only abstract functions to the server core.

@dfn{Shared library} or @dfn{Public API} is a front-end of Speech Daemon
that provides polished functions programmers should
use to send commands to the server.

@dfn{SSIP} is the layer (communication protocol) between server side
(server core) and client side (shared library). It stands for Speech
Synthesis Internet Protocol.  Client programs should never use it
directly.

@dfn{Socket} or @dfn{File descriptor} represents the particular connection
between a client and server. In C, it's and integer variable.

@node Server core, Output modules, Definitions, Internal structure
@section Server core

see sources, I'll try to write this section soon

@node Output modules,  , Server core, Internal structure
@section Output modules

@findex synthesizer_write()
@findex synthesizer_stop()
@findex synthesizer_is_speaking()
@findex synthesizer_close()

Output modules for Speech Daemon have the form of a glib plug-ins
located in src/modules/. Each output module is a data
structure composed of some parameters and pointers to it's functions.

@example
typedef struct @{
  gchar    *name;
  gchar    *description;
  gchar    *filename;
  gint     (*write)    (const gchar *, gint, void*);
  gint     (*stop)     (void);
  gint     (*is_speaking) (void);
  gint     (*close)    (void);
@} OutputModule;
@end example

This structure is defined in @file{intl/modules.h} and therefore 
this header must be included in every plugin source code.

@example
#include "modules.h"
@end example

Also one other file called @file{intl/fdset.h} where the FDSetElement
structure is defined must be included to be able to handle the
different speech synthesis settings.

@example
#include "fdset.h"
@end example

Each output module has associated a module_init function
that is called at the starting of Speech Daemon. After doing
the necessary initialization, it must return a filled structure
of the type OutputModule (defined above).

@example
OutputModule *module_init(void)@{
        ...
        return &module_flite;
@}
@end example

Now what are the 4 functions: flite_write, flite_stop, flite_is_speaking
and flite_close? This is the core of every output module and you
have to define their bodies in the source code of your plug-in.

@deffn {Output module functions}  gint synthesizer_write const gchar *data, gint len, TFDSetElement* set

This is the function where actual speech output is produced. It is called
every time Speech Daemon decides to send a message to synthesis. The data
of lenght @var{len} are passed in @var{data}. Additionally, the structure
containing settings associated to this particular message is passed,
however only few options are important for output modules.

Each output module should take care of setting the output device to these parameters
(the other ones are handled independently in other parts of Speech Daemon):
@itemize @bullet
@item (signed int) set->speed
@item (signed int) set->pitch
@item (char*) set->language
@item (int) set->voice_type
@end itemize

Speed and pitch are values between -100 and 100 included. 0 is the default
value that represents normal speech flow. So -100 is the slowest (or lowest)
and +100 is the fastest (or highest) speech.

(We should estabilish a constant scale refered to some text, standard speeds
and different associated times. This will probably be the work of the person
who will program the first real output module. We need to chose some longer
text, decide what speed of reading we consider 0 and what we consider, say,
+-50, measure the times needed to read it at these speeds and put it there
in documentation as our standard scale.)

The language parameter is given as a null-terminated string containnig 
the name of the language in english in lowercase (e.g. "english", "czech", "spanish").

voice_type is used only when the output module supports more types of voices
for this particular language. The values represent (from @file{intl/fdset.h})
@example
typedef enum @{
    MALE = 0,
    FEMALE = 1,
    CHILD_MALE = 2,
    CHILD_FEMALE = 3
@}EVoiceType;
@end example
We can consider also other voice types. 

This function should return 0 if it fails and some non-0 value
if the delivery to the synthesis is succesful. Formerly we thought
that it should return the number of bytes written, but it's still
not clear how to handle messages that have to be divided in more
parts (for example if the output device has a finite size buffer).

@end deffn

@deffn {Output module functions}  {gint synthesizer_stop} void

This function should stop the synthesis of the currently spoken message
immediately and throw away the rest of the message.

It should return 0 on succes, -1 otherwise.

@end deffn

@deffn {Output module functions}  {gint synthesizer_is_speaking} void

This function is very important to let Speech Daemon know how to
regulate the speech flow between different queues, programs and even
other synthesizers. On calling it, the output module must decide
whether there is currently any output beeing produced in the speakers.

This can be a very hard problem and it's not clear how to do it
with diferent synthesizers. If it's not possible to return an exact
value, at least some estimate should be calculated. But such an inacurate
value can highly reduce the usefulness of an even otherwise very good
plug-in. To some degree, this is still an open question.

It should return 0 if the synthesis is silent, 1 if it's speaking.

@end deffn

@deffn {Output module functions}  {gint synthesizer_close} void

This function is called when Speech Daemon terminates. There are no
special requierements on what the output plug-in should do.

It should return 0 on succes, -1 otherwise.

@end deffn

@node Public API, SSIP, Internal structure, Top
@chapter Public API

@findex spd_init()
@findex spd_close()
@findex spd_say()
@findex spd_sayf()
@findex spd_stop()
@findex int spd_pause()
@findex int spd_resume()
@findex spd_command_line()
@findex spd_stop_fd()
@findex spd_pause_fd()
@findex spd_resume_fd()
@findex spd_history_select_client()
@findex spd_get_client_list()
@findex spd_get_message_list_fd()

@ifinfo
@verbatiminclude ../src/clients/libspeechd.h
@end ifinfo

@c don't know if it's really The Right Thing, but I often miss *any*
@c description in the header files, and there is literate programming
@c on the other side. if you have a reason why it's not ok, I'm open
@c to the idea to move it all here

@node SSIP, Priorities, Public API, Top
@chapter Speech Synthesis Internet Protocol (SSIP)

Clients communicate with Speech Daemon via the Speech Synthesis
Internet Protocol (SSIP).  The protocol is the actual interface to
Speech Daemon.

Usually, you don't need to use SSIP directly, you can use one of the
programming interfaces, see @ref{Public API}, wrapping SSIP with
programming library calls.  This is a recommended way of communication
with Speech Daemon.  However, in case your programming environment is
not supported by any of the provided interfaces or you prefer to
communicate with Speech Daemon directly for any reason, you can find
the complete SSIP description here.

@menu
* General rules::               Overall conventions applying to SSIP.
* SSIP commands::               Complete reference of SSIP commands.
* Return codes::                List of SSIP result codes.
* Sample SSIP relation::        An example session.
@end menu

@node General rules, SSIP commands, SSIP, SSIP
@section General rules

SSIP communicates with the clients through a defined set of text
commands, in the way usual in common Internet protocols.  The
characters sent to and from Speech Daemon are encoded using the UTF-8
encoding.

Each SSIP command, unless specified otherwise, consists of exactly one
line.  The line is sent in the following format:

@example
@var{command} @var{ARG} ...
@end example

where @var{command} is a case insensitive command name and @var{arg}s
are its arguments separated by spaces.  The command arguments which
come from a defined set of values are case insensitive as well.  The
number of arguments is dependent on the particular command and there
can be commands having no arguments.

All lines of SSIP input and output must be ended with the pair of
carriage return and line feed characters, in this order.

When you connect to Speech Daemon, you should at least set your client
name, through the @code{SET CLIENT_NAME} command, @ref{Parameter
setting commands}.  This is important to get a proper identification
of your client --- to allow managing it from the control center
application and to identify it in a message history browser.  You
might want to set other connection parameters as well, look for more
details in @ref{Parameter setting commands}.

Connection to Speech Daemon is preferably closed by issuing the
@code{QUIT} command, see @ref{Other commands}.

SSIP is a synchronous protocol --- you send commands and only after a
complete response from SSIP arrives back you are allowed to send the
next command.  Usually, the connection to Speech Daemon remains open
during the whole run of the particular client application.  If you
close the connection and open it again, you must set all the
previously set parameters again, Speech Daemon doesn't store session
parameters between connections.

The protocol allows you to perform commands regarding other currently
connected or previously connected clients.  This allows you to write a
control application managing or browsing all the messages received by
the current Speech Daemon process.  The mechanism is completely
relaxed, there are no restrictions on accessing messages of other
clients and users and managing some aspects of their sound output.

SSIP replies of Speech Daemon are of the following format:

@example
@var{ccc}-line 1
@var{ccc}-line 2
...
@var{ccc}-line @var{n}-1
@var{ddd} line @var{n}
@end example

where @var{n} is a positive integer, and @var{ccc} and @var{ddd} are
three-digit long numeric codes identifying the result of the command.
The last line determines the overall result of the command, the result
code is followed by an English message describing the result of the
action in a human readable form.

@node SSIP commands, Return codes, General rules, SSIP
@section SSIP commands

Commands recognized by SSIP can be divided into several groups: Speech
synthesis and sound output commands, speech control commands,
parameter setting commands, commands retrieving information about
current client and server settings, command handling the message
history, and other commands.  Each of these command groups is
described in one of the following sections.

In the command descriptions, the command is written together with its
arguments.  Optional arguments are enclosed by square brackets
(@code{[} and @code{]}), alternatives are separated by the vertical
rule (@code{|}) and are grouped within braces (@code{@{} and
@code{@}}) or square brackets for mandatory or optional arguments
respectively, literal arguments values are typeset in lower letters
(they are case insensitive), and variable arguments are typeset
@var{like this}.

@menu
* Speech synthesis and sound output commands::  
* Speech output control commands::  
* Parameter setting commands::  
* Information retrieval commands::  
* History handling commands::   
* Other commands::              
@end menu

@node Speech synthesis and sound output commands, Speech output control commands, SSIP commands, SSIP commands
@subsection Speech synthesis and sound output

These commands invoke Speech Daemon mechanisms transforming given data
and parameters into an audio sample and sending it onto an audio
device.  The particular way how the message is handled is defined by
the Speech Daemon configuration mechanism (@pxref{Configuration}) and
are out of scope of SSIP.

@table @code
@item SPEAK
Start receiving a text message and synthesize it.  After sending a
reply to the command, Speech Daemon waits for the text of the
message.  The text can spread over any number of lines and is
finished by an end of line marker followed by the line containing the
single character @code{.} (dot).  Thus the complete character sequence
closing the input text is @code{CR LF . CR LF}.  If any line within
the sent text starts with a dot, an extra dot is prepended before it.

During reception of the text message, Speech Daemon doesn't send
response to the particular lines sent.  The response line is sent only
immediately after the @code{SPEAK} command and after receiving the
closing dot line.

Speech Daemon can start speech synthesis as soon as a sufficient
amount of the text arrives, it generally needn't (but may) wait until
the end of data marker is received.

There is no explicit upper limit on the size of the text, but the
server administrator may set one in the configuration or the limit can
be enforced by available system resources.  If the limit is exceeded,
the whole text is accepted, but its exceeding part is ignored and an
error response code is returned after processing the final dot line.

This command, unlike all other commands, stores the received text into
the message history.

@item LETTER @var{char}
Speak letter @var{char}.  @var{char} can be any character
representable by the UTF-8 encoding.

This command is intended to be used for speaking single letters,
e.g.@ when reading a character under cursor or when spelling words.

@item KEY @var{key-name}
Speak key identified by @var{key-name}.  @var{key-name} is ???

This command is intended to be used for speaking keys pressed by the
user.

@item SOUND_ICON @var{icon-name}
Send a sound identified by @var{icon-name} to the audio output.
@var{icon-name} is a symbolic name of the given sound from the
standard set listed in @ref{Standard sound icons}, or another name
from the particular Speech Daemon sound icon configuration.
@end table

@node Speech output control commands, Parameter setting commands, Speech synthesis and sound output commands, SSIP commands
@subsection Controlling speech output

These commands can stop or resume speech or audio output.  They all
affect only the synthesis process and output to a sound device, they
do not affect the message history.

@table @code
@item STOP [ @var{id} | all ]
Immediately stop outputing the current message (whatever it is ---
text, letter, key, or sound icon) from the identified client, if any
is being output.  If there is no command argument given, last message
from the current client connection is stopped.  If @var{id} is given,
it must be a positive integer, stop the currently processed message
from the client connection identified by @var{id}; if there is none
such, do nothing.  If @code{all} is given as the command argument,
stop currently output message or messages from all the clients.

@item CANCEL [ @var{id} | all ]
This command is the same as @code{SPEAK}, with the exception that it
stops not yet output messages as well.  All currently queued messages
are stored into the message history without being sent to the audio
output device.

@item PAUSE [ @var{id} | all ]
Stop audio output immediately, but do not discard anything.  All the
currently output and currently or later queued messages are postponed
and saved for later processing, until the corresponding @code{RESUME}
command is received.

The meaning of the command arguments is the same as in the @code{STOP}
command.

@item RESUME [ @var{id} | all ]
Cancel the effect of the previously issued @code{PAUSE} command.
Note that messages of the priority 3 received during the pause are not
output (but they remain stored in the message history).

It is an error to send the @code{RESUME} command when the output
corresponding to the given argument is not paused by a previous
invocation of the @code{PAUSE} command.  Such an error is signalled by
a @code{4XX} return code.

The meaning of the command arguments is the same as in the @code{STOP}
command.
@end table

@node Parameter setting commands, Information retrieval commands, Speech output control commands, SSIP commands
@subsection Parameter setting

The @code{SET} command sets various control parameters of Speech
Daemon.  The parameter is always denoted by the first command
argument.

All the settings take effect to the client connection (only) and until
the parameter setting is changed by another invocation of the
appropriate @code{SET} command or until the connection is closed.

@table @code
@item SET CLIENT_NAME @var{user}:@var{client}:@var{component}
Set client's name.  Client name consists of the user name, client
(application) identification, and the identification of the component
of the client (application).  Each of the parts of the client name may
contain only alphanumeric characters.

For example, for a client called @code{lynx} that creates Speech
Daemon connection for its command processing, the name could be set in
the following way:

@example
SET CLIENT_NAME joe:lynx:cmd_processing
@end example

The client name is used in the server configuration settings, client
listings and message history handling.  All its three parts can be
arbitrary, but it's important to define and follow rules for each
application supporting Speech Daemon, so that a Speech Daemon user can
configure all the aspects of the speech output easily.

Usually, this command should be sent as the very first command when a
new connection to Speech Daemon is established.

@item SET LANGUAGE @var{language}
Set recommended language for this client to @var{language}.
@var{language} is the name of the language according to RFC 1766.

For example, to set the preferred language to Czech, you send the
following command:
@example
SET LANGUAGE cs
@end example

@item SET PRIORITY @var{n}
Set message priority to @var{n}.  @var{n} must be one of the values
@code{1}, @code{2}, and @code{3}.

@item SET PUNCTUATION @{ all | some | none @}
Set punctuation mode to the given value.  @code{all} means read all
punctuation characters, @code{none} read no punctuation characters,
@code{some} means read only punctuation characters given in the
server configuration.

@item SET SPELLING_TABLE @var{table}
Set spelling table to @var{table}.  The list of the available spelling
tables on the server can be obtained via the @code{LIST
SPELLING_TABLES} command, see @ref{Information retrieval commands}.

@item SET TEXT_TABLE @var{table}
Similar to @code{SET SPELLING_TABLE}, but sets the text mapping table
instead.

@item SET SOUND_TABLE @var{table}
Similar to @code{SET SPELLING_TABLE}, but sets the sound mapping table
instead.

@item SET VOICE ???
???

@item SET RATE @var{n}
Set the rate of speech.  @var{n} is an integer value within the range
from -100 to 100, with 0 corresponding to the default rate of the
current speech synthesis output module, lower values meaning slower
speech and higher values meaning faster speech.

@item SET PITCH @var{n}
Set the pitch of speech.  @var{n} is an integer value within the range
from -100 to 100, with 0 corresponding to the default pitch of the
current speech synthesis output module, lower values meaning lower
pitch and higher values meaning higher pitch.
@end table

@node Information retrieval commands, History handling commands, Parameter setting commands, SSIP commands
@subsection Retrieving information

The @code{LIST} command serves for retrieving information that can be
presented to the user for selection of the values to the @code{SET}
command.  The information listed is selected according to the first
argument of the @code{LIST} command.

@table @code
@item LIST SPELLING_TABLES
List the names of all the spelling tables available on the server.
Each table name is listed on a separate line.

Example Speech Daemon response:

@example
200-sptable2
200-sptable1
200-sptable44
200-special-table
200 OK Tables listed.
@end example

@item LIST TEXT_TABLES
Similar to @code{LIST SPELLING_TABLES}, but lists the names of the
available text mapping tables.

@item LIST SOUND_TABLES
Similar to @code{LIST SPELLING_TABLES}, but lists the names of the
available sound mapping tables.
@end table

@node History handling commands, Other commands, Information retrieval commands, SSIP commands
@subsection History handling

History is handled by the @code{HISTORY} command.  It can take many
forms, described below, that allow browsing, retrieving and repeating
stored messages.

There is always a so called @dfn{history cursor} pointing on some
message in the history.  You can move it across history messages and
retrieve the message the cursor is pointing to, using the
@code{HISTORY CURSOR} set of command arguments described below.

@table @code
@item HISTORY GET CLIENT_LIST
List known client names, their ids and status.  Each connection is
listed on a separate line in the following format:

@example
@var{id} @var{name} @var{status}
@end example

where @var{id} is a client id that can be used in other history
handling requests or in the speech output control commands
(@pxref{Speech output control commands}), @var{name} is the client
name as set through the @code{SET CLIENT_NAME} command, and
@var{status} is @code{1} for connected clients and @code{0} for
disconnected clients.  @var{id}s are unique within a single run of
Speech Daemon.

Sample reply of Speech Daemon:

@example
240-0 speechd_client:main 0
240-1 speechd_client:status 0
240-2 unknown:unknown 1
240 OK CLIENTS LIST SENT
@end example

@item HISTORY GET CLIENT_MESSAGES @var{id} @var{from} @var{num}
List ids of messages sent by the client identified by @var{id}.
@var{num} messages is listed, starting from the message @var{from}.
The first message is ???.  Messages are sorted ???.  If the given
range exceeds the range of available messages, no error is signalled,
that exceeding part is ignored.

Each message id is listed, together with its client name, on a
separate line, in the following format:

@example
@var{id} @var{client-name}
@end example

All the message ids in the history, regardless of clients that issued
them, are unique within a single run of Speech Daemon.

@item HISTORY GET LAST
List the id of the last processed (?) message.

The id is listed, together with its client name, on a separate line of
the following format:

@example
@var{id} @var{client-name}
@end example

@item HISTORY SORT
Sorts the history buffer of messages. ???

@item HISTORY CURSOR GET
Get the id of the message the history cursor is pointing on.

The id is listed on a separate line.  Sample Speech Daemon reply to
this command:

@example
243-42
243 OK CURSOR POSITION RETURNED
@end example

@item HISTORY CURSOR SET @var{id} @{ first | last | pos @var{n} @}
Set the history cursor to the given position.  @var{id} is the id of
the client whose history should be used.  The argument @code{first}
asks to set the cursor on the first position and the argument
@code{last} asks to set the cursor on the last positition of the
history of the given client.  If the argument @code{pos} is used, the
position is set to @var{n}, where @var{n} is a positive (???) integer.
It is an error if @var{id} doesn't identify any client or if @var{n}
doesn't point to any existing position in the history.

As for the order and numbering of the messages in the history, the
same rules apply as in @code{HISTORY GET CLIENT_MESSAGES}, see above.

@item HISTORY CURSOR @{ next | prev @}
Move the cursor one position forward or backward between the messages
of the currently specified client (??? which one ???).

@item HISTORY SAY ID @var{id}
Speak the message identified by @var{id}.

@item HISTORY SAY TEXT "@var{text}"
Speak @var{text} as a normal synthesized text, but without storing it
into history.  @var{text} may / may not ???

This command is useful for history processing clients.
@end table

@node Other commands,  , History handling commands, SSIP commands
@subsection Other commands

@table @code
@item QUIT
Close the connection.

@item HELP
Print a short list of all SSIP commands, as a multiline message.
@end table

@node Return codes, Sample SSIP relation, SSIP commands, SSIP
@section Return codes

Each line of the SSIP output starts with a three-digit numeric code of
the form @var{NXX} where @var{N} determines the result group and
@var{xx} denotes the finer classification of the result.

SSIP defines the following result groups:

@table @var
@item 1xx
Informative response --- general information about the protocol, help
messages.

@item 2xx
Operation was completely successful.

@item 3xx
Server error, problem on the server side.

@item 4xx
Client error, invalid arguments or parameters received.

@item 5xx
Client error, invalid command syntax, unparsable input.
@end table

Result groups @var{1xx} and @var{2xx} correspond to successful
actions, other groups to unsuccessful actions.  Only the groups
defined here may be returned from the Speech Daemon.

Currenty, only the meaning of the first digit of the result code is
defined, the last two digits are insignificant and can be of any
value.  Clients shouldn't rely on the unspecified digits in any way.
If you are going to write your own SSIP implementation, please consult
the authors of Speech Daemon to define more precise set of return
codes.

@node Sample SSIP relation,  , Return codes, SSIP
@section Example of an SSIP relation

The following example illustrates a sample relation with SSIP.  The
client connects to the Speech Daemon, sets all the common parameters,
sends two text messages, looks up them in the message history,
instructs Speech Daemon to repeat the second message, and closes the
connection.  Lines starting with a numeric code are response lines of
the server, other lines are the lines sent by the client.

@example
SET CLIENT_NAME joe:vi:default
208 OK CLIENT NAME SET
SET PRIORITY 2
202 OK PRIORITY SET
SPEAK
230 OK RECEIVING DATA
Hello, I'm a Speech Daemon communication example!
How are you?
.
225 OK MESSAGE QUEUED
SPEAK
230 OK RECEIVING DATA
Still there?
.
225 OK MESSAGE QUEUED
HISTORY GET CLIENT_LIST
240-1 joe:vi:default 1
240 OK CLIENTS LIST SENT
HISTORY GET CLIENT_MESSAGES joe:vi:default 2 1
500 ERR INVALID COMMAND
...
QUIT
231 HAPPY HACKING
@end example


@node Priorities, Multiple output modules, SSIP, Top
@chapter Priorities
@cindex priorities

The possibility to distinguish between several message priority levels
seems to be essential. Each message sent by client to speech server
should have a priority level assigned.

Speech Daemon provides the system of three priority levels. Every message will
either contain explicit level information, or the default value will be
considered. There is a separate message queue for each of the levels.
The behavior is as follows:

@section Level 1
These messages will be said immediately as they come to server.
They are never interrupted. These messages should be as short
as possible, because they block the output of all other
messages. When several concurrent messages are received by
server, they are queued and said in the order, they came.
When a new message of level 1 comes during lower level
message is spoken, lower level message is canceled and removed
from the queue (this message is allready stored in the history)

@section Level 2
Second level messages are said in the moment, when there is no
message of level 1 queued. Several messages of level 2 are said
in the order, they are received (queued, but in their own
queue). This is the default level for messages without explicit
level information.

@section Level 3
Third level messages are only said, when there are no messages
of any higher level queued. If there are level 3 messages beeing
said or waiting in queues, they are interrupted by the last
incomming level 3 message and this one is said, in other words,
level 3 is interrupting itself.

@section How to use them wisely

Example uses for level @strong{one} are:
 
@itemize
@item error messages
@item very important messages
@item ...
@end itemize

Example uses for level @strong{two} are:

@itemize
@item regular program messages
@item menus
@item text the user is working on
@item ...
@end itemize

Example uses for level @strong{three} are:

@itemize
@item less important status information
@item letters when typing input
@item ...
@end itemize


@node Multiple output modules, Messsage history, Priorities, Top
@chapter Multiple output modules
@cindex output module
@cindex different synthesizers

Speech Daemon supports concurrent use of multiple output modules.
In the case these output modules provide good synchronization,
you can combine them in reading messages. For example if module1 can
speak English and Czech while module2 speaks only German, the idea
is that if there is something message in German, module2 is used,
while module1 is used for the other languages. These rules for
selection of output modules can be influenced through the configuration
file @file{speechd.conf}.

If you want to compile and use a new output module, you should place
it in @file{src/modules} in your source directory of Speech Daemon and
add it to @file{src/modules/Makefile.am}. You can compile and install
it by typing: @code{make; su root; make install}. The last step you
have to do is to let Speech Daemon know you want to use this new
module by adding a line to @file{speechd.conf} in your configuration directory
@example
AddModule module_name
@end example
and possibly also changing the line
@example
DefaultModule new_module
@end example
to make it default.

@xref{Output modules}.

@node Messsage history, Speech parameters, Multiple output modules, Top
@chapter Message history

@node Speech parameters, Configuration, Messsage history, Top
@chapter Speech parameters
@cindex Speech parameters
@cindex Settings

@section Language selection

Various synthesizers provide different sets of possible
languages, they are allowed to speak. We must be able to
receive a request for setting particular language (using
ISO language code) and reply, if the language is supported.

@section Speed

Sped of the speech is supported by all synthesizers, but the
values and their ranges differ. Each output module is
responsible to set the speed to the value, best responding to
current setting. This may be a little bit difficult, because
there is no exact scale. We could take some longer english
paragraph and take it as a base for our new scale. If this
paragraph is said in eg. ten secconds, this means speed = 100,
if it is said in twenty seconds, speed = 200. This way, we
can coordinate  diferent scales quite preciselly (the paragraph
should be long enough).

@section Punctuation mode

Punctuation mode describes the way, in which the synthesizer
works with non-alphanumeric characters. Most synthesizers
support several punctuation modes. We will support a reasonable
superset of those modes, which may be implemented in device
driver, when not supported by hardware.

@section Prosody
Prosody setting allows us, to distinguish interpunction
characters in spoken text, as we are familiar in normal speech.
This means the way, we pronounce the text with interrogation mark,
coma, dot etc.

@section Pitch
Pitch is the voice frequency. We face the similar problems
here, as with Speed setting.

@section Voice type
Most synthesizers provide several voice types, such as male,
female, child etc. The set is again different for each
of the devices. Speech Daemon should try to find the nearest
possible (if the request is child female and it's not available,
we will try to use adult female rather then adult male).

@section Spelling
Spelling mode is provided by nearly all devices and is also
easy to emulate in output module.

@section Capital letters recognition
That is again a widely supported feature. However it is
desirable to support this internally, using the
sound icons feature, but this  requires a good possibility of
synchronization, which is not  possible with all devices.


@node Configuration, Standard sound icons, Speech parameters, Top
@chapter Configuration
@cindex configuration
@cindex default values

Speech Daemon can be configured on several levels.
There is a configuration file where permanent settings
are stored, but user and applications can also change
the majority of parameters on-fly by calling Speech Daemon
functions. The third level of configuration can't be
changed and it's given by the capabilities of each output
device (each output module for each output device reports
it's capabilities when it's loaded into Speech Daemon).

We use DotConf for the permanent textfile-based configuration.
See @file{speechd.conf}.

Other parts of this manual deal with the runtime configuration.


@node Standard sound icons, Copying, Configuration, Top
@appendix Standard sound icons

There are none currently.


@node Copying, Concept index, Standard sound icons, Top
@appendix GNU GENERAL PUBLIC LICENSE
@center Version 2, June 1991

@include gpl.texi


@node Concept index,  , Copying, Top
@unnumbered Concept index

@cindex tail recursion
@printindex cp

@contents
@bye

@c speechd.texi ends here

